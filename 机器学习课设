'''读取数据'''
data = pd.read_csv('train.csv')   #DataFrame类型
del data['datetime']
del data['item']
'''整理训练集合'''
ItemNum=18
X_Train=[]        #训练样本features集合
Y_Train=[]        #训练样本目标PM2.5集合
for i in range(int(len(data)/ItemNum)):
    day = data[i*ItemNum:(i+1)*ItemNum] #一天的观测数据
    for j in range(15):
        x = day.iloc[:, j:j + 9]
        y = int(day.iloc[9,j+9])
        X_Train.append(x)
        Y_Train.append(y)
        
        '''绘制散点图'''
x_AMB_TEMP=[]
x_CH4=[]
x_CO=[]
x_NMHC=[]
y=[]
for i in range(len(Y_Train)):
    y.append(Y_Train[i])
    x=X_Train[i]
    #求各测项的平均值
    x_AMB_TEMP_sum=0
    x_CH4_sum=0
    x_CO_sum=0
    x_NMHC_sum=0
    for j in range(9):
        x_AMB_TEMP_sum=x_AMB_TEMP_sum+x.iloc[0,j]
        x_CH4_sum = x_CH4_sum + x.iloc[1, j]
        x_CO_sum = x_CO_sum + x.iloc[2, j]
        x_NMHC_sum = x_NMHC_sum + x.iloc[3, j]
    x_AMB_TEMP.append(x_AMB_TEMP_sum / 9)
    x_CH4.append(x_CH4_sum / 9)
    x_CO.append(x_CO_sum / 9)
    x_NMHC.append(x_NMHC_sum / 9)
plt.figure(figsize=(10, 6))
plt.subplot(2, 2, 1)
plt.title('AMB_TEMP')
plt.scatter(x_AMB_TEMP, y)
plt.subplot(2, 2, 2)
plt.title('CH4')
plt.scatter(x_CH4, y)
plt.subplot(2, 2, 3)
plt.title('CO')
plt.scatter(x_CO, y)
plt.subplot(2, 2, 4)
plt.title('NMHC')
plt.scatter(x_NMHC, y)
plt.show(）
'''小批量梯度下降'''
dict={0:8,1:8,2:8,3:8,4:8,5:8,6:8,7:8,8:8,9:9,10:9,11:9,12:9,13:9,14:9,15:9,16:9,17:9,18:12,19:12,20:12,21:12,22:12,23:12,24:12,25:12,26:12}
iteration_count = 10000   #迭代次数
learning_rate = 0.000001  #学习速率
b=0.0001    #初始化偏移项
parameters=[0.001]*27     #初始化27个参数
loss_history=[]
for i in range(iteration_count):
    loss=0
    b_grad=0
    w_grad=[0]*27
    examples=list(randint(0, len(X_Train)-1) for index in range(100))
    for j in range(100):
        index=examples.pop()
        day = X_Train[index]
        partsum = b+parameters[0]*day.iloc[8,0]+parameters[1]*day.iloc[8,1]+parameters[2]*day.iloc[8,2]+parameters[3]*day.iloc[8,3]+parameters[4]*day.iloc[8,4]+parameters[5]*day.iloc[8,5]+parameters[6]*day.iloc[8,6]+parameters[7]*day.iloc[8,7]+parameters[8]*day.iloc[8,8]+parameters[9]*day.iloc[9,0]+parameters[10]*day.iloc[9,1]+parameters[11]*day.iloc[9,2]+parameters[12]*day.iloc[9,3]+parameters[13]*day.iloc[9,4]+parameters[14]*day.iloc[9,5]+parameters[15]*day.iloc[9,6]+parameters[16]*day.iloc[9,7]+parameters[17]*day.iloc[9,8]+parameters[18]*day.iloc[12,0]+parameters[19]*day.iloc[12,1]+parameters[20]*day.iloc[12,2]+parameters[21]*day.iloc[12,3]+parameters[22]*day.iloc[12,4]+parameters[23]*day.iloc[12,5]+parameters[24]*day.iloc[12,6]+parameters[25]*day.iloc[12,7]+parameters[26]*day.iloc[12,8]-Y_Train[index]
        loss=loss + partsum * partsum
        b_grad = b_grad + partsum
        for k in range(27):
            w_grad[k]=w_grad[k]+ partsum * day.iloc[dict[k],k % 9]
    loss_history.append(loss/2)
    #更新参数
    b = b - learning_rate * b_grad
    for t in range(27):
        parameters[t] = parameters[t] - learning_rate * w_grad[t]
